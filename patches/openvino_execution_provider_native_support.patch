diff --git include/onnxruntime/core/session/onnxruntime_c_api.h include/onnxruntime/core/session/onnxruntime_c_api.h
index 8986cdc..5a40dc6 100644
--- include/onnxruntime/core/session/onnxruntime_c_api.h
+++ include/onnxruntime/core/session/onnxruntime_c_api.h
@@ -384,7 +384,7 @@ typedef struct OrtTensorRTProviderOptions {
 /// </summary>
 typedef struct OrtOpenVINOProviderOptions {
 #ifdef __cplusplus
-  OrtOpenVINOProviderOptions() : device_type{}, enable_vpu_fast_compile{}, device_id{}, num_of_threads{}, use_compiled_network{}, blob_dump_path{} {}
+  OrtOpenVINOProviderOptions() : device_type{}, enable_vpu_fast_compile{}, device_id{}, num_of_threads{}, use_compiled_network{}, blob_dump_path{}, enable_optimization{}, optimization_workdir{} {}
 #endif
   const char* device_type;                // CPU_FP32, GPU_FP32, GPU_FP16, MYRIAD_FP16, VAD-M_FP16 or VAD-F_FP32
   unsigned char enable_vpu_fast_compile;  // 0 = false, nonzero = true
@@ -392,6 +392,8 @@ typedef struct OrtOpenVINOProviderOptions {
   size_t num_of_threads;               // 0 uses default number of threads
   unsigned char use_compiled_network;  // 0 = false, nonzero = true
   const char* blob_dump_path;          // path is set to empty by default
+  unsigned char enable_optimization;
+  const char* optimization_workdir;
 } OrtOpenVINOProviderOptions;
 
 struct OrtApi;
diff --git onnxruntime/core/providers/openvino/backend_utils.cc onnxruntime/core/providers/openvino/backend_utils.cc
index f683f00..9d6af55 100644
--- onnxruntime/core/providers/openvino/backend_utils.cc
+++ onnxruntime/core/providers/openvino/backend_utils.cc
@@ -6,6 +6,9 @@
 #include <memory>
 #include <sstream>
 #include <fstream>
+#include <cstdlib>
+#include <sys/wait.h>
+#include <filesystem>
 
 #include <inference_engine.hpp>
 
@@ -134,15 +137,53 @@ CreateCNNNetwork(const ONNX_NAMESPACE::ModelProto& model_proto, const GlobalCont
 #else
   //ReadNetwork() API flow will be used in OpenVINO-EP starting from OpenVINO 2021.4
   InferenceEngine::CNNNetwork cnn_network;
-  const std::string model = model_proto.SerializeAsString();
-  InferenceEngine::Blob::Ptr blob = {nullptr};
-  try {
-    cnn_network = global_context.ie_core.ReadNetwork(model, blob);
-    LOGS_DEFAULT(INFO) << "Read network Done";
-  } catch (const Exception& e) {
-    ORT_THROW(log_tag + "[OpenVINO-EP] Exception while Reading network: " + std::string(e.what()));
-  } catch (...) {
-    ORT_THROW(log_tag + "[OpenVINO-EP] Unknown exception while Reading network");
+
+  bool is_network_optimized = false;
+  if (global_context.enable_optimization) {
+    try {
+      const std::string workdir = global_context.optimization_workdir;
+      const std::string dump_name = "model";
+      std::filesystem::create_directory(workdir + "/");
+      std::ofstream onnx_file(workdir + "/" + dump_name + ".onnx", std::ofstream::binary);
+      model_proto.SerializeToOstream(onnx_file);
+      onnx_file.close();
+
+      const std::string input_model = "'" + workdir + "/" + dump_name + ".onnx'";
+      const std::string output_dir = "'" + workdir + "'";
+      const std::string cmd = "mo" \
+                        " --input_model " + input_model \
+                      + " --output_dir " + output_dir \
+                      + " --model_name '" + dump_name + "'" \
+                        " --framework 'onnx' > /dev/null 2>&1";
+      int rcode = std::system(cmd.c_str());
+      rcode = (rcode != -1 && rcode != 127 && WIFEXITED(rcode)) ? WEXITSTATUS(rcode) : -1;
+      if (rcode != 0) {
+          throw std::runtime_error("No model optimizer found");
+      }
+
+      cnn_network = global_context.ie_core.ReadNetwork(workdir + "/" + dump_name+ ".xml", workdir + "/" + dump_name + ".bin");
+      is_network_optimized = true;
+      LOGS_DEFAULT(INFO) << "Optimize and Read network Done";
+    } catch (const Exception& e) {
+      LOGS_DEFAULT(ERROR) << "Failed to optimize and read network: " + std::string(e.what());
+      is_network_optimized = false;
+    } catch (...) {
+      LOGS_DEFAULT(ERROR) << "Unknown exception while optimizing and reading network";
+      is_network_optimized = false;
+    }
+  }
+
+  if (not global_context.enable_optimization or (global_context.enable_optimization and not is_network_optimized)) {
+    const std::string model = model_proto.SerializeAsString();
+    InferenceEngine::Blob::Ptr blob = {nullptr};
+    try {
+      cnn_network = global_context.ie_core.ReadNetwork(model, blob);
+      LOGS_DEFAULT(INFO) << "Read network Done";
+    } catch (const Exception& e) {
+      ORT_THROW(log_tag + "[OpenVINO-EP] Exception while Reading network: " + std::string(e.what()));
+    } catch (...) {
+      ORT_THROW(log_tag + "[OpenVINO-EP] Unknown exception while Reading network");
+    }
   }
   ng_function = cnn_network.getFunction();
 #endif
@@ -154,24 +195,6 @@ CreateCNNNetwork(const ONNX_NAMESPACE::ModelProto& model_proto, const GlobalCont
     ng_function->validate_nodes_and_infer_types();
   }
 
-  if (!global_context.is_wholly_supported_graph) {
-    std::map<std::string, std::string> result_to_output;
-    for (auto& result : ng_function->get_results()) {
-      result_to_output[result->get_friendly_name()] = result->input_value(0).get_node_shared_ptr()->get_friendly_name();
-    }
-
-    ngraph::pass::ConstantFolding().run_on_function(ng_function);
-    auto& results = const_cast<::ngraph::ResultVector&>(ng_function->get_results());
-    size_t index = results.size() - 1;
-    for (auto it = results.rbegin(); it != results.rend(); ++it) {
-      if (auto const_node = std::dynamic_pointer_cast<ngraph::op::Constant>((*it)->input_value(0).get_node_shared_ptr())) {
-        const_outputs_map[result_to_output.at((*it)->get_friendly_name())] = const_node;
-        results.erase(results.begin() + index);
-      }
-      --index;
-    }
-  }
-
   try {
     return std::make_shared<InferenceEngine::CNNNetwork>(ng_function);
   } catch (const Exception& e) {
diff --git onnxruntime/core/providers/openvino/contexts.h onnxruntime/core/providers/openvino/contexts.h
index 6453c41..ac85083 100644
--- onnxruntime/core/providers/openvino/contexts.h
+++ onnxruntime/core/providers/openvino/contexts.h
@@ -14,11 +14,13 @@ struct GlobalContext {
   bool is_wholly_supported_graph = false;
   bool enable_vpu_fast_compile = false;
   bool use_compiled_network = false;
+  bool enable_optimization = false;
   size_t num_of_threads;
   std::string device_type;
   std::string precision_str;
   std::string device_id;
   std::string blob_dump_path;
+  std::string optimization_workdir;
   std::vector<bool> deviceAvailableList = {true, true, true, true, true, true, true, true};
   std::vector<std::string> deviceTags = {"0", "1", "2", "3", "4", "5", "6", "7"};
   std::string onnx_model_name;
diff --git onnxruntime/core/providers/openvino/openvino_execution_provider.cc onnxruntime/core/providers/openvino/openvino_execution_provider.cc
index c8e261a..731e35c 100644
--- onnxruntime/core/providers/openvino/openvino_execution_provider.cc
+++ onnxruntime/core/providers/openvino/openvino_execution_provider.cc
@@ -19,7 +19,9 @@ OpenVINOExecutionProvider::OpenVINOExecutionProvider(const OpenVINOExecutionProv
   openvino_ep::BackendManager::GetGlobalContext().precision_str = info.precision_;
   openvino_ep::BackendManager::GetGlobalContext().enable_vpu_fast_compile = info.enable_vpu_fast_compile_;
   openvino_ep::BackendManager::GetGlobalContext().use_compiled_network = info.use_compiled_network_;
+  openvino_ep::BackendManager::GetGlobalContext().enable_optimization = info.enable_optimization_;
   openvino_ep::BackendManager::GetGlobalContext().blob_dump_path = info.blob_dump_path_;
+  openvino_ep::BackendManager::GetGlobalContext().optimization_workdir = info.optimization_workdir_;
 
   if ((int)info.num_of_threads_ <= 0) {
     openvino_ep::BackendManager::GetGlobalContext().num_of_threads = 8;
diff --git onnxruntime/core/providers/openvino/openvino_execution_provider.h onnxruntime/core/providers/openvino/openvino_execution_provider.h
index 469ce4e..368f3d8 100644
--- onnxruntime/core/providers/openvino/openvino_execution_provider.h
+++ onnxruntime/core/providers/openvino/openvino_execution_provider.h
@@ -58,9 +58,11 @@ struct OpenVINOExecutionProviderInfo {
   size_t num_of_threads_;
   bool use_compiled_network_;
   std::string blob_dump_path_;
+  bool enable_optimization_;
+  std::string optimization_workdir_;
 
-  explicit OpenVINOExecutionProviderInfo(std::string dev_type, bool enable_vpu_fast_compile, std::string dev_id, size_t num_of_threads, bool use_compiled_network, std::string blob_dump_path)
-      : enable_vpu_fast_compile_(enable_vpu_fast_compile), device_id_(dev_id), num_of_threads_(num_of_threads), use_compiled_network_(use_compiled_network), blob_dump_path_(blob_dump_path) {
+  explicit OpenVINOExecutionProviderInfo(std::string dev_type, bool enable_vpu_fast_compile, std::string dev_id, size_t num_of_threads, bool use_compiled_network, std::string blob_dump_path, bool enable_optimization, std::string optimization_workdir)
+      : enable_vpu_fast_compile_(enable_vpu_fast_compile), device_id_(dev_id), num_of_threads_(num_of_threads), use_compiled_network_(use_compiled_network), blob_dump_path_(blob_dump_path), enable_optimization_(enable_optimization), optimization_workdir_(optimization_workdir) {
     if (dev_type == "") {
       LOGS_DEFAULT(INFO) << "[OpenVINO-EP]"
                          << "No runtime device selection option provided.";
@@ -128,7 +130,7 @@ struct OpenVINOExecutionProviderInfo {
                        << "Choosing Device: " << device_type_ << " , Precision: " << precision_;
   }
   OpenVINOExecutionProviderInfo() {
-    OpenVINOExecutionProviderInfo("", false, "", 0, false,"");
+    OpenVINOExecutionProviderInfo("", false, "", 0, false,"", false, "");
   }
 };
 
diff --git onnxruntime/core/providers/openvino/openvino_provider_factory.cc onnxruntime/core/providers/openvino/openvino_provider_factory.cc
index 4bb6509..0088e1c 100644
--- onnxruntime/core/providers/openvino/openvino_provider_factory.cc
+++ onnxruntime/core/providers/openvino/openvino_provider_factory.cc
@@ -9,11 +9,13 @@ namespace onnxruntime {
 struct OpenVINOProviderFactory : IExecutionProviderFactory {
   OpenVINOProviderFactory(const char* device_type, bool enable_vpu_fast_compile,
                           const char* device_id, size_t num_of_threads,
-                          bool use_compiled_network, const char* blob_dump_path)
-      : enable_vpu_fast_compile_(enable_vpu_fast_compile), num_of_threads_(num_of_threads), use_compiled_network_(use_compiled_network) {
+                          bool use_compiled_network, const char* blob_dump_path,
+                          bool enable_optimization, const char* optimization_workdir)
+      : enable_vpu_fast_compile_(enable_vpu_fast_compile), num_of_threads_(num_of_threads), use_compiled_network_(use_compiled_network), enable_optimization_(enable_optimization) {
     device_type_ = (device_type == nullptr) ? "" : device_type;
     device_id_ = (device_id == nullptr) ? "" : device_id;
     blob_dump_path_ = (blob_dump_path == nullptr) ? "" : blob_dump_path;
+    optimization_workdir_ = (optimization_workdir == nullptr) ? "" : optimization_workdir;
   }
   ~OpenVINOProviderFactory() override {
   }
@@ -27,16 +29,18 @@ struct OpenVINOProviderFactory : IExecutionProviderFactory {
   size_t num_of_threads_;
   bool use_compiled_network_;
   std::string blob_dump_path_;
+  bool enable_optimization_;
+  std::string optimization_workdir_;
 };
 
 std::unique_ptr<IExecutionProvider> OpenVINOProviderFactory::CreateProvider() {
-  OpenVINOExecutionProviderInfo info(device_type_, enable_vpu_fast_compile_, device_id_, num_of_threads_, use_compiled_network_, blob_dump_path_);
+  OpenVINOExecutionProviderInfo info(device_type_, enable_vpu_fast_compile_, device_id_, num_of_threads_, use_compiled_network_, blob_dump_path_, enable_optimization_, optimization_workdir_);
   return std::make_unique<OpenVINOExecutionProvider>(info);
 }
 
 std::shared_ptr<IExecutionProviderFactory> CreateExecutionProviderFactory_OpenVINO(
-    const char* device_type, bool enable_vpu_fast_compile, const char* device_id, size_t num_of_threads, bool use_compiled_network, const char* blob_dump_path) {
-  return std::make_shared<onnxruntime::OpenVINOProviderFactory>(device_type, enable_vpu_fast_compile, device_id, num_of_threads, use_compiled_network, blob_dump_path);
+    const char* device_type, bool enable_vpu_fast_compile, const char* device_id, size_t num_of_threads, bool use_compiled_network, const char* blob_dump_path, bool enable_optimization, const char* optimization_workdir) {
+  return std::make_shared<onnxruntime::OpenVINOProviderFactory>(device_type, enable_vpu_fast_compile, device_id, num_of_threads, use_compiled_network, blob_dump_path, enable_optimization, optimization_workdir);
 }
 
 }  // namespace onnxruntime
@@ -54,7 +58,7 @@ struct OpenVINO_Provider : Provider {
 
   std::shared_ptr<IExecutionProviderFactory> CreateExecutionProviderFactory(const void* void_params) override {
     auto& params = *reinterpret_cast<const OrtOpenVINOProviderOptions*>(void_params);
-    return std::make_shared<OpenVINOProviderFactory>(params.device_type, params.enable_vpu_fast_compile, params.device_id, params.num_of_threads, params.use_compiled_network, params.blob_dump_path);
+    return std::make_shared<OpenVINOProviderFactory>(params.device_type, params.enable_vpu_fast_compile, params.device_id, params.num_of_threads, params.use_compiled_network, params.blob_dump_path, params.enable_optimization, params.optimization_workdir);
   }
 
   void Shutdown() override {
 
diff --git onnxruntime/python/onnxruntime_pybind_schema.cc onnxruntime/python/onnxruntime_pybind_schema.cc
index 9caddaf..9fafcaa 100644
--- onnxruntime/python/onnxruntime_pybind_schema.cc
+++ onnxruntime/python/onnxruntime_pybind_schema.cc
@@ -44,7 +44,7 @@ void addGlobalSchemaFunctions(pybind11::module& m) {
             onnxruntime::CreateExecutionProviderFactory_Dnnl(1),
 #endif
 #ifdef USE_OPENVINO
-            onnxruntime::CreateExecutionProviderFactory_OpenVINO(openvino_device_type, false, "", 8, false, ""),
+            onnxruntime::CreateExecutionProviderFactory_OpenVINO(openvino_device_type, false, "", 8, false, "", false, ""),
 #endif
 #ifdef USE_TENSORRT
             onnxruntime::CreateExecutionProviderFactory_Tensorrt(
@@ -209,4 +209,4 @@ void addOpSchemaSubmodule(py::module& m) {
       .value("EXPERIMENTAL", ONNX_NAMESPACE::OpSchema::SupportType::EXPERIMENTAL);
 }
 }
-}  // namespace onnxruntime
\ No newline at end of file
+}  // namespace onnxruntime
diff --git onnxruntime/python/onnxruntime_pybind_state.cc onnxruntime/python/onnxruntime_pybind_state.cc
index bae9a1f..422aa39 100644
--- onnxruntime/python/onnxruntime_pybind_state.cc
+++ onnxruntime/python/onnxruntime_pybind_state.cc
@@ -571,6 +571,17 @@ std::unique_ptr<IExecutionProvider> CreateExecutionProviderInstance(
         } else if (option.first == "blob_dump_path") {
           blob_dump_path = option.second;
           params.blob_dump_path = blob_dump_path.c_str();
+        } else if (option.first == "enable_optimization") {
+          if (option.second == "True") {
+            params.enable_optimization = true;
+          } else if (option.second == "False") {
+            params.enable_optimization = false;
+          } else {
+            ORT_THROW("Invalid value passed for enable_optimization: ", option.second);
+          }
+        } else if (option.first == "optimization_workdir") {
+          std::string optimization_workdir = option.second;
+          params.optimization_workdir = optimization_workdir.c_str();
         } else {
           ORT_THROW("Invalid OpenVINO EP option: ", option.first);
         }
